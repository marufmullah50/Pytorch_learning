<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PyTorch Machine Learning Templates</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }
        
        header p {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .content {
            padding: 40px;
        }
        
        .section {
            margin-bottom: 50px;
            padding: 30px;
            background: #f8f9fa;
            border-radius: 15px;
            border-left: 5px solid #667eea;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        
        .section:hover {
            transform: translateX(5px);
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }
        
        .section h2 {
            color: #667eea;
            margin-bottom: 20px;
            font-size: 1.8em;
            display: flex;
            align-items: center;
        }
        
        .section-number {
            background: #667eea;
            color: white;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: inline-flex;
            align-items: center;
            justify-content: center;
            margin-right: 15px;
            font-weight: bold;
        }
        
        .code-block {
            background: #282c34;
            color: #abb2bf;
            padding: 20px;
            border-radius: 10px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.5;
            box-shadow: inset 0 2px 10px rgba(0,0,0,0.3);
        }
        
        .code-block pre {
            margin: 0;
        }
        
        .explanation {
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin-top: 20px;
            border-left: 4px solid #28a745;
        }
        
        .explanation h3 {
            color: #28a745;
            margin-bottom: 15px;
            font-size: 1.3em;
        }
        
        .explanation ul, .explanation ol {
            margin-left: 20px;
            margin-top: 10px;
        }
        
        .explanation li {
            margin-bottom: 8px;
        }
        
        .models-box {
            background: #fff3cd;
            padding: 15px;
            border-radius: 8px;
            margin-top: 15px;
            border: 2px dashed #ffc107;
        }
        
        .models-box h4 {
            color: #856404;
            margin-bottom: 10px;
        }
        
        .models-box ul {
            margin-left: 20px;
        }
        
        .keyword {
            color: #c678dd;
            font-weight: bold;
        }
        
        .string {
            color: #98c379;
        }
        
        .comment {
            color: #5c6370;
            font-style: italic;
        }
        
        .function {
            color: #61afef;
        }
        
        footer {
            background: #2c3e50;
            color: white;
            text-align: center;
            padding: 20px;
            margin-top: 40px;
        }
        
        @media (max-width: 768px) {
            .container {
                margin: 10px;
            }
            
            header h1 {
                font-size: 1.8em;
            }
            
            .content {
                padding: 20px;
            }
            
            .section {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ðŸš€ PyTorch Machine Learning Templates</h1>
            <p>Complete Guide with 8 Real-World Implementations</p>
        </header>
        
        <div class="content">
            <!-- Section 1: Linear Regression -->
            <div class="section">
                <h2><span class="section-number">1</span>Linear Regression</h2>
                <div class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler

<span class="comment"># Set device</span>
device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)
<span class="function">print</span>(<span class="string">f'Using device: {device}'</span>)

<span class="comment"># Load data</span>
df = pd.read_csv(<span class="string">'data.csv'</span>)
X = df.iloc[:, :3].values  <span class="comment"># First 3 columns as input</span>
y = df.iloc[:, 3].values.reshape(-1, 1)  <span class="comment"># 4th column as output</span>

<span class="comment"># Split and scale data</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_train = scaler_X.fit_transform(X_train)
X_test = scaler_X.transform(X_test)
y_train = scaler_y.fit_transform(y_train)
y_test = scaler_y.transform(y_test)

<span class="comment"># Convert to tensors</span>
X_train = torch.FloatTensor(X_train).to(device)
X_test = torch.FloatTensor(X_test).to(device)
y_train = torch.FloatTensor(y_train).to(device)
y_test = torch.FloatTensor(y_test).to(device)

<span class="comment"># Define model</span>
<span class="keyword">class</span> <span class="function">LinearRegressionModel</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, input_dim):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, 1)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="keyword">return</span> self.linear(x)

model = LinearRegressionModel(3).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

<span class="comment"># Training loop</span>
epochs = 1000
<span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    
    <span class="keyword">if</span> (epoch + 1) % 100 == 0:
        <span class="function">print</span>(<span class="string">f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}'</span>)

<span class="comment"># Evaluation</span>
model.eval()
<span class="keyword">with</span> torch.no_grad():
    predictions = model(X_test)
    test_loss = criterion(predictions, y_test)
    <span class="function">print</span>(<span class="string">f'Test Loss: {test_loss.item():.4f}'</span>)</pre>
                </div>
                
                <div class="explanation">
                    <h3>ðŸ“– Explanation</h3>
                    <p><strong>Linear Regression</strong> predicts continuous numerical values by finding a linear relationship between input features and output.</p>
                    <ul>
                        <li><strong>Device Setup:</strong> Automatically uses GPU if available for faster computation</li>
                        <li><strong>Data Preprocessing:</strong> StandardScaler normalizes features to have mean=0 and std=1</li>
                        <li><strong>Model Architecture:</strong> Single linear layer that learns weights and bias</li>
                        <li><strong>Loss Function:</strong> MSE (Mean Squared Error) measures prediction accuracy</li>
                        <li><strong>Optimizer:</strong> Adam adjusts weights to minimize loss</li>
                    </ul>
                    
                    <div class="models-box">
                        <h4>ðŸ”§ Alternative Models for Regression:</h4>
                        <ul>
                            <li><strong>Ridge Regression:</strong> Add L2 regularization to prevent overfitting</li>
                            <li><strong>Lasso Regression:</strong> Add L1 regularization for feature selection</li>
                            <li><strong>Polynomial Regression:</strong> Add polynomial features for non-linear relationships</li>
                            <li><strong>ElasticNet:</strong> Combination of L1 and L2 regularization</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Section 2: Classification -->
            <div class="section">
                <h2><span class="section-number">2</span>Classification</h2>
                <div class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, LabelEncoder

<span class="comment"># Set device</span>
device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)
<span class="function">print</span>(<span class="string">f'Using device: {device}'</span>)

<span class="comment"># Load data</span>
df = pd.read_csv(<span class="string">'data.csv'</span>)
X = df.iloc[:, :3].values
y = df.iloc[:, 3].values

<span class="comment"># Encode labels if categorical</span>
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
num_classes = len(label_encoder.classes_)

<span class="comment"># Split and scale</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

<span class="comment"># Convert to tensors</span>
X_train = torch.FloatTensor(X_train).to(device)
X_test = torch.FloatTensor(X_test).to(device)
y_train = torch.LongTensor(y_train).to(device)
y_test = torch.LongTensor(y_test).to(device)

<span class="comment"># Define model</span>
<span class="keyword">class</span> <span class="function">ClassificationModel</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, input_dim, num_classes):
        super(ClassificationModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, num_classes)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        <span class="keyword">return</span> x

model = ClassificationModel(3, num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

<span class="comment"># Training loop</span>
epochs = 100
<span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    
    <span class="keyword">if</span> (epoch + 1) % 10 == 0:
        <span class="function">print</span>(<span class="string">f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}'</span>)

<span class="comment"># Evaluation</span>
model.eval()
<span class="keyword">with</span> torch.no_grad():
    predictions = model(X_test)
    _, predicted = torch.max(predictions, 1)
    accuracy = (predicted == y_test).sum().item() / len(y_test)
    <span class="function">print</span>(<span class="string">f'Test Accuracy: {accuracy*100:.2f}%'</span>)</pre>
                </div>
                
                <div class="explanation">
                    <h3>ðŸ“– Explanation</h3>
                    <p><strong>Classification</strong> predicts discrete categories/classes based on input features.</p>
                    <ul>
                        <li><strong>Label Encoding:</strong> Converts categorical outputs to numerical labels</li>
                        <li><strong>Network Architecture:</strong> Multiple hidden layers with ReLU activation</li>
                        <li><strong>Dropout:</strong> Randomly drops neurons during training to prevent overfitting</li>
                        <li><strong>CrossEntropyLoss:</strong> Standard loss for multi-class classification</li>
                        <li><strong>Evaluation:</strong> Uses accuracy metric (correct predictions / total)</li>
                    </ul>
                    
                    <div class="models-box">
                        <h4>ðŸ”§ Alternative Models for Classification:</h4>
                        <ul>
                            <li><strong>Logistic Regression:</strong> Simple linear classifier</li>
                            <li><strong>Support Vector Machine (SVM):</strong> Maximum margin classifier</li>
                            <li><strong>Random Forest:</strong> Ensemble of decision trees</li>
                            <li><strong>Gradient Boosting (XGBoost, LightGBM):</strong> Powerful ensemble methods</li>
                            <li><strong>K-Nearest Neighbors (KNN):</strong> Instance-based learning</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Section 3: ANN Regression -->
            <div class="section">
                <h2><span class="section-number">3</span>ANN Regression (Deep Neural Network)</h2>
                <div class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler
<span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, TensorDataset

<span class="comment"># Set device</span>
device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)
<span class="function">print</span>(<span class="string">f'Using device: {device}'</span>)

<span class="comment"># Load data</span>
df = pd.read_csv(<span class="string">'data.csv'</span>)
X = df.iloc[:, :3].values
y = df.iloc[:, 3].values.reshape(-1, 1)

<span class="comment"># Split and scale</span>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_train = scaler_X.fit_transform(X_train)
X_test = scaler_X.transform(X_test)
y_train = scaler_y.fit_transform(y_train)
y_test = scaler_y.transform(y_test)

<span class="comment"># Create DataLoader</span>
train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

<span class="comment"># Define ANN model</span>
<span class="keyword">class</span> <span class="function">ANNRegression</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, input_dim):
        super(ANNRegression, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)
        self.batch_norm1 = nn.BatchNorm1d(128)
        self.batch_norm2 = nn.BatchNorm1d(64)
        self.batch_norm3 = nn.BatchNorm1d(32)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        x = self.fc1(x)
        x = self.batch_norm1(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        x = self.fc2(x)
        x = self.batch_norm2(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        x = self.fc3(x)
        x = self.batch_norm3(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        x = self.fc4(x)
        <span class="keyword">return</span> x

model = ANNRegression(3).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)

<span class="comment"># Training loop</span>
epochs = 100
<span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):
    model.train()
    train_loss = 0
    <span class="keyword">for</span> X_batch, y_batch <span class="keyword">in</span> train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    
    train_loss /= len(train_loader)
    scheduler.step(train_loss)
    
    <span class="keyword">if</span> (epoch + 1) % 10 == 0:
        <span class="function">print</span>(<span class="string">f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}'</span>)

<span class="comment"># Evaluation</span>
model.eval()
test_loss = 0
<span class="keyword">with</span> torch.no_grad():
    <span class="keyword">for</span> X_batch, y_batch <span class="keyword">in</span> test_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        predictions = model(X_batch)
        loss = criterion(predictions, y_batch)
        test_loss += loss.item()

test_loss /= len(test_loader)
<span class="function">print</span>(<span class="string">f'Test Loss: {test_loss:.4f}'</span>)</pre>
                </div>
                
                <div class="explanation">
                    <h3>ðŸ“– Explanation</h3>
                    <p><strong>ANN (Artificial Neural Network) Regression</strong> uses multiple hidden layers to learn complex non-linear patterns in data.</p>
                    <ul>
                        <li><strong>Deep Architecture:</strong> 4 layers (128â†’64â†’32â†’1) capture complex patterns</li>
                        <li><strong>Batch Normalization:</strong> Normalizes activations, speeds up training, reduces overfitting</li>
                        <li><strong>Dropout:</strong> Randomly deactivates 30% of neurons to prevent overfitting</li>
                        <li><strong>DataLoader:</strong> Efficiently loads data in batches for training</li>
                        <li><strong>Learning Rate Scheduler:</strong> Reduces learning rate when loss plateaus</li>
                    </ul>
                    
                    <div class="models-box">
                        <h4>ðŸ”§ Alternative Deep Learning Architectures:</h4>
                        <ul>
                            <li><strong>ResNet (Residual Networks):</strong> Skip connections for very deep networks</li>
                            <li><strong>DenseNet:</strong> Each layer connects to all previous layers</li>
                            <li><strong>Wide & Deep:</strong> Combines memorization and generalization</li>
                            <li><strong>Attention Mechanisms:</strong> Focuses on important features</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Section 4: CNN for Image Classification -->
            <div class="section">
                <h2><span class="section-number">4</span>CNN for Image Classification</h2>
                <div class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms
<span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader
<span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder

<span class="comment"># Set device</span>
device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)
<span class="function">print</span>(<span class="string">f'Using device: {device}'</span>)

<span class="comment"># Data transforms</span>
train_transform = transforms.Compose([
    transforms.Resize((244, 244)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
    transforms.Resize((244, 244)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

<span class="comment"># Load datasets</span>
train_dataset = ImageFolder(<span class="string">'image_dataset/train'</span>, transform=train_transform)
test_dataset = ImageFolder(<span class="string">'image_dataset/test'</span>, transform=test_transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

<span class="comment"># Define CNN model</span>
<span class="keyword">class</span> <span class="function">CNN</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, num_classes):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(128 * 30 * 30, 512)
        self.fc2 = nn.Linear(512, num_classes)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
        self.batch_norm1 = nn.BatchNorm2d(32)
        self.batch_norm2 = nn.BatchNorm2d(64)
        self.batch_norm3 = nn.BatchNorm2d(128)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        x = self.pool(self.relu(self.batch_norm1(self.conv1(x))))
        x = self.pool(self.relu(self.batch_norm2(self.conv2(x))))
        x = self.pool(self.relu(self.batch_norm3(self.conv3(x))))
        x = x.view(-1, 128 * 30 * 30)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        <span class="keyword">return</span> x

model = CNN(num_classes=12).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

<span class="comment"># Training loop</span>
epochs = 50
<span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    
    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    accuracy = 100 * correct / total
    <span class="keyword">if</span> (epoch + 1) % 5 == 0:
        <span class="function">print</span>(<span class="string">f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%'</span>)

<span class="comment"># Evaluation</span>
model.eval()
correct = 0
total = 0
<span class="keyword">with</span> torch.no_grad():
    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

<span class="function">print</span>(<span class="string">f'Test Accuracy: {100 * correct / total:.2f}%'</span>)</pre>
                </div>
                
                <div class="explanation">
                    <h3>ðŸ“– Explanation</h3>
                    <p><strong>CNN (Convolutional Neural Network)</strong> is specialized for image data, using convolutional layers to detect patterns and features.</p>
                    <ul>
                        <li><strong>Convolutional Layers:</strong> Extract spatial features (edges, textures, shapes)</li>
                        <li><strong>Max Pooling:</strong> Reduces spatial dimensions, retains important features</li>
                        <li><strong>Data Augmentation:</strong> Random flips and rotations increase training diversity</li>
                        <li><strong>Batch Normalization:</strong> Stabilizes training in convolutional layers</li>
                        <li><strong>Image Normalization:</strong> Uses ImageNet mean/std for better convergence</li>
                    </ul>
                    
                    <div class="models-box">
                        <h4>ðŸ”§ Popular CNN Architectures:</h4>
                        <ul>
                            <li><strong>LeNet:</strong> Classic architecture for digit recognition</li>
                            <li><strong>AlexNet:</strong> First deep CNN to win ImageNet</li>
                            <li><strong>VGG:</strong> Very deep with small 3x3 filters</li>
                            <li><strong>ResNet:</strong> Skip connections, 50-152 layers</li>
                            <li><strong>Inception:</strong> Multiple filter sizes in parallel</li>
                            <li><strong>EfficientNet:</strong> Optimized accuracy and efficiency</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Section 5: TorchVision Models -->
            <div class="section">
                <h2><span class="section-number">5</span>Using TorchVision Pre-trained Models</h2>
                <div class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models
<span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms
<span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader
<span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder

<span class="comment"># Set device</span>
device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)
<span class="function">print</span>(<span class="string">f'Using device: {device}'</span>)

<span class="comment"># Data transforms</span>
train_transform = transforms.Compose([
    transforms.Resize((244, 244)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
    transforms.Resize((244, 244)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

<span class="comment"># Load datasets</span>
train_dataset = ImageFolder(<span class="string">'image_dataset/train'</span>, transform=train_transform)
test_dataset = ImageFolder(<span class="string">'image_dataset/test'</span>, transform=test_transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=32, num_workers=4)

<span class="comment"># Load pre-trained ResNet18</span>
model = models.resnet18(pretrained=True)

<span class="comment"># Freeze all layers</span>
<span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():
    param.requires_grad = False

<span class="comment"># Replace final layer for 12 classes</span>
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 12)

model = model.to(device)

<span class="comment"># Loss and optimizer</span>
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)  <span class="comment"># Only train final layer</span>

<span class="comment"># Training loop</span>
epochs = 20
<span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    
    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    accuracy = 100 * correct / total
    <span class="function">print</span>(<span class="string">f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss/len(train_loader):.4f}, Acc: {accuracy:.2f}%'</span>)

<span class="comment"># Evaluation</span>
model.eval()
correct = 0
total = 0
<span class="keyword">with</span> torch.no_grad():
    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

<span class="function">print</span>(<span class="string">f'Test Accuracy: {100 * correct / total:.2f}%'</span>)</pre>
                </div>
                
                <div class="explanation">
                    <h3>ðŸ“– Explanation</h3>
                    <p><strong>TorchVision Models</strong> provides pre-trained models on ImageNet dataset, allowing quick deployment with excellent performance.</p>
                    <ul>
                        <li><strong>Pre-trained Weights:</strong> Model already learned features from millions of images</li>
                        <li><strong>Feature Extraction:</strong> Freeze early layers, only train final classifier</li>
                        <li><strong>ImageFolder:</strong> Convenient dataset loader for folder-organized images</li>
                        <li><strong>Faster Training:</strong> Much faster than training from scratch</li>
                        <li><strong>Better Performance:</strong> Pre-trained features generalize well</li>
                    </ul>
                    
                    <div class="models-box">
                        <h4>ðŸ”§ Available TorchVision Models:</h4>
                        <ul>
                            <li><strong>ResNet (18, 34, 50, 101, 152):</strong> Deep residual networks</li>
                            <li><strong>VGG (11, 13, 16, 19):</strong> Very deep convolutional networks</li>
                            <li><strong>DenseNet (121, 161, 169, 201):</strong> Dense connections</li>
                            <li><strong>MobileNet V2/V3:</strong> Efficient for mobile devices</li>
                            <li><strong>EfficientNet (B0-B7):</strong> Scaled architectures</li>
                            <li><strong>Vision Transformer (ViT):</strong> Transformer-based vision model</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Section 6: Download and Use Different Pre-trained Models -->
            <div class="section">
                <h2><span class="section-number">6</span>Download Different Pre-trained Models</h2>
                <div class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models
<span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader
<span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms
<span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder

<span class="comment"># Set device</span>
device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)
<span class="function">print</span>(<span class="string">f'Using device: {device}'</span>)

<span class="comment"># Data preparation</span>
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

train_dataset = ImageFolder(<span class="string">'image_dataset/train'</span>, transform=transform)
test_dataset = ImageFolder(<span class="string">'image_dataset/test'</span>, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

<span class="comment"># Download and use different models</span>
<span class="keyword">def</span> <span class="function">get_model</span>(model_name, num_classes=12):
    <span class="keyword">if</span> model_name == <span class="string">'resnet50'</span>:
        model = models.resnet50(pretrained=True)
        num_features = model.fc.in_features
        model.fc = nn.Linear(num_features, num_classes)
    
    <span class="keyword">elif</span> model_name == <span class="string">'vgg16'</span>:
        model = models.vgg16(pretrained=True)
        num_features = model.classifier[6].in_features
        model.classifier[6] = nn.Linear(num_features, num_classes)
    
    <span class="keyword">elif</span> model_name == <span class="string">'densenet121'</span>:
        model = models.densenet121(pretrained=True)
        num_features = model.classifier.in_features
        model.classifier = nn.Linear(num_features, num_classes)
    
    <span class="keyword">elif</span> model_name == <span class="string">'mobilenet_v2'</span>:
        model = models.mobilenet_v2(pretrained=True)
        num_features = model.classifier[1].in_features
        model.classifier[1] = nn.Linear(num_features, num_classes)
    
    <span class="keyword">elif</span> model_name == <span class="string">'efficientnet_b0'</span>:
        model = models.efficientnet_b0(pretrained=True)
        num_features = model.classifier[1].in_features
        model.classifier[1] = nn.Linear(num_features, num_classes)
    
    <span class="keyword">else</span>:
        <span class="keyword">raise</span> ValueError(<span class="string">f'Model {model_name} not supported'</span>)
    
    <span class="keyword">return</span> model

<span class="comment"># Choose model</span>
model_name = <span class="string">'resnet50'</span>  <span class="comment"># Change this to try different models</span>
model = get_model(model_name).to(device)

<span class="comment"># Freeze feature extraction layers</span>
<span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():
    <span class="keyword">if</span> <span class="string">'fc'</span> <span class="keyword">not in</span> name <span class="keyword">and</span> <span class="string">'classifier'</span> <span class="keyword">not in</span> name:
        param.requires_grad = False

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(filter(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters()), lr=0.001)

<span class="comment"># Training function</span>
<span class="keyword">def</span> <span class="function">train_model</span>(model, train_loader, criterion, optimizer, epochs=10):
    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):
        model.train()
        train_loss = 0
        correct = 0
        total = 0
        
        <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        accuracy = 100 * correct / total
        <span class="function">print</span>(<span class="string">f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss/len(train_loader):.4f}, Acc: {accuracy:.2f}%'</span>)

<span class="comment"># Evaluation function</span>
<span class="keyword">def</span> <span class="function">evaluate_model</span>(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    <span class="keyword">with</span> torch.no_grad():
        <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    accuracy = 100 * correct / total
    <span class="function">print</span>(<span class="string">f'Test Accuracy: {accuracy:.2f}%'</span>)
    <span class="keyword">return</span> accuracy

<span class="comment"># Train and evaluate</span>
train_model(model, train_loader, criterion, optimizer, epochs=15)
evaluate_model(model, test_loader)</pre>
                </div>
                
                <div class="explanation">
                    <h3>ðŸ“– Explanation</h3>
                    <p><strong>Model Hub</strong> approach allows easy experimentation with different architectures to find the best one for your task.</p>
                    <ul>
                        <li><strong>Model Selection:</strong> Easy switching between architectures</li>
                        <li><strong>Automatic Download:</strong> PyTorch downloads pre-trained weights automatically</li>
                        <li><strong>Unified Interface:</strong> Same training code works for all models</li>
                        <li><strong>Architecture Comparison:</strong> Test multiple models quickly</li>
                        <li><strong>Best Practices:</strong> Freeze backbone, train classifier</li>
                    </ul>
                    
                    <div class="models-box">
                        <h4>ðŸ”§ Model Selection Guide:</h4>
                        <ul>
                            <li><strong>ResNet50/101:</strong> Good balance of accuracy and speed</li>
                            <li><strong>VGG16:</strong> Simple architecture, high memory usage</li>
                            <li><strong>DenseNet:</strong> Efficient, lower parameters, good accuracy</li>
                            <li><strong>MobileNet:</strong> Fast inference, mobile-friendly</li>
                            <li><strong>EfficientNet:</strong> Best accuracy vs efficiency trade-off</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Section 7: Transfer Learning -->
            <div class="section">
                <h2><span class="section-number">7</span>Transfer Learning (Fine-tuning)</h2>
                <div class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models
<span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader
<span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms
<span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder

<span class="comment"># Set device</span>
device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)
<span class="function">print</span>(<span class="string">f'Using device: {device}'</span>)

<span class="comment"># Data transforms with augmentation</span>
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

train_dataset = ImageFolder(<span class="string">'image_dataset/train'</span>, transform=train_transform)
test_dataset = ImageFolder(<span class="string">'image_dataset/test'</span>, transform=test_transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=32, num_workers=4)

<span class="comment"># Load pre-trained model</span>
model = models.resnet50(pretrained=True)

<span class="comment"># STAGE 1: Feature Extraction (freeze all but final layer)</span>
<span class="function">print</span>(<span class="string">"Stage 1: Training final layer only"</span>)
<span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():
    param.requires_grad = False

num_features = model.fc.in_features
model.fc = nn.Sequential(
    nn.Dropout(0.5),
    nn.Linear(num_features, 512),
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(512, 12)
)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)

<span class="comment"># Train stage 1</span>
epochs_stage1 = 10
<span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs_stage1):
    model.train()
    train_loss = 0
    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    
    <span class="function">print</span>(<span class="string">f'Stage 1 - Epoch [{epoch+1}/{epochs_stage1}], Loss: {train_loss/len(train_loader):.4f}'</span>)

<span class="comment"># STAGE 2: Fine-tuning (unfreeze last few layers)</span>
<span class="function">print</span>(<span class="string">"\nStage 2: Fine-tuning entire network"</span>)

<span class="comment"># Unfreeze all layers</span>
<span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():
    param.requires_grad = True

<span class="comment"># Different learning rates for different layers</span>
optimizer = torch.optim.Adam([
    {<span class="string">'params'</span>: model.layer4.parameters(), <span class="string">'lr'</span>: 1e-4},
    {<span class="string">'params'</span>: model.fc.parameters(), <span class="string">'lr'</span>: 1e-3}
], lr=1e-5)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">'min'</span>, patience=3)

<span class="comment"># Train stage 2</span>
epochs_stage2 = 20
best_accuracy = 0
<span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs_stage2):
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    
    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    train_accuracy = 100 * correct / total
    avg_loss = train_loss / len(train_loader)
    scheduler.step(avg_loss)
    
    <span class="comment"># Validation</span>
    model.eval()
    val_correct = 0
    val_total = 0
    <span class="keyword">with</span> torch.no_grad():
        <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()
    
    val_accuracy = 100 * val_correct / val_total
    
    <span class="function">print</span>(<span class="string">f'Stage 2 - Epoch [{epoch+1}/{epochs_stage2}]'</span>)
    <span class="function">print</span>(<span class="string">f'Train Loss: {avg_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%'</span>)
    
    <span class="comment"># Save best model</span>
    <span class="keyword">if</span> val_accuracy > best_accuracy:
        best_accuracy = val_accuracy
        torch.save(model.state_dict(), <span class="string">'best_model.pth'</span>)
        <span class="function">print</span>(<span class="string">f'Best model saved with accuracy: {best_accuracy:.2f}%'</span>)

<span class="function">print</span>(<span class="string">f'\nBest Validation Accuracy: {best_accuracy:.2f}%'</span>)</pre>
                </div>
                
                <div class="explanation">
                    <h3>ðŸ“– Explanation</h3>
                    <p><strong>Transfer Learning</strong> leverages knowledge from pre-trained models, fine-tuning them for specific tasks with less data and training time.</p>
                    <ul>
                        <li><strong>Two-Stage Training:</strong> First train classifier, then fine-tune entire network</li>
                        <li><strong>Differential Learning Rates:</strong> Lower LR for pre-trained layers, higher for new layers</li>
                        <li><strong>Gradual Unfreezing:</strong> Prevents catastrophic forgetting of learned features</li>
                        <li><strong>Model Checkpointing:</strong> Save best model based on validation accuracy</li>
                        <li><strong>Learning Rate Scheduling:</strong> Adaptive learning rate based on performance</li>
                    </ul>
                    
                    <div class="models-box">
                        <h4>ðŸ”§ Transfer Learning Strategies:</h4>
                        <ul>
                            <li><strong>Feature Extraction:</strong> Freeze all layers, train only classifier</li>
                            <li><strong>Fine-tuning Last Layers:</strong> Unfreeze last few conv blocks</li>
                            <li><strong>Full Fine-tuning:</strong> Train entire network with small LR</li>
                            <li><strong>Progressive Unfreezing:</strong> Gradually unfreeze layers during training</li>
                            <li><strong>Discriminative Learning Rates:</strong> Different LR for different layers</li>
                        </ul>
                        <h4>ðŸ’¡ Fine-tuning Tips:</h4>
                        <ul>
                            <li>Use smaller learning rates (1e-4 to 1e-5) for pre-trained layers</li>
                            <li>Apply more aggressive data augmentation</li>
                            <li>Use larger dropout for regularization</li>
                            <li>Monitor validation loss to prevent overfitting</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Section 8: Time Series Analysis with LSTM -->
            <div class="section">
                <h2><span class="section-number">8</span>Time Series Analysis (LSTM)</h2>
                <div class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> TimeSeriesSplit
<span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, TensorDataset

<span class="comment"># Set device</span>
device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)
<span class="function">print</span>(<span class="string">f'Using device: {device}'</span>)

<span class="comment"># Load time series data</span>
df = pd.read_csv(<span class="string">'timeseries_data.csv'</span>)
df[<span class="string">'date'</span>] = pd.to_datetime(df[<span class="string">'date'</span>])  <span class="comment"># First column: date</span>
df = df.sort_values(<span class="string">'date'</span>)

<span class="comment"># Extract features and targets</span>
dates = df[<span class="string">'date'</span>].values
targets = df.iloc[:, 1:4].values  <span class="comment"># 3 output columns</span>

<span class="comment"># Scale data</span>
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(targets)

<span class="comment"># Create sequences</span>
<span class="keyword">def</span> <span class="function">create_sequences</span>(data, seq_length):
    X, y = [], []
    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    <span class="keyword">return</span> np.array(X), np.array(y)

seq_length = 30  <span class="comment"># Use 30 time steps to predict next</span>
X, y = create_sequences(scaled_data, seq_length)

<span class="comment"># Time Series Cross-Validation Split</span>
tscv = TimeSeriesSplit(n_splits=5)
<span class="keyword">for</span> train_idx, test_idx <span class="keyword">in</span> tscv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    <span class="keyword">break</span>  <span class="comment"># Use first split for demo</span>

<span class="comment"># Convert to tensors</span>
X_train = torch.FloatTensor(X_train).to(device)
X_test = torch.FloatTensor(X_test).to(device)
y_train = torch.FloatTensor(y_train).to(device)
y_test = torch.FloatTensor(y_test).to(device)

<span class="comment"># Create DataLoaders</span>
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=64)

<span class="comment"># Define LSTM model</span>
<span class="keyword">class</span> <span class="function">LSTMModel</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, 
                           batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="comment"># Initialize hidden state and cell state</span>
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        
        <span class="comment"># Forward propagate LSTM</span>
        out, _ = self.lstm(x, (h0, c0))
        
        <span class="comment"># Get output from last time step</span>
        out = self.fc(out[:, -1, :])
        <span class="keyword">return</span> out

<span class="comment"># Model parameters</span>
input_dim = 3  <span class="comment"># 3 output features</span>
hidden_dim = 128
num_layers = 2
output_dim = 3

model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)

<span class="comment"># Training loop</span>
epochs = 100
best_loss = float(<span class="string">'inf'</span>)

<span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):
    model.train()
    train_loss = 0
    
    <span class="keyword">for</span> X_batch, y_batch <span class="keyword">in</span> train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        train_loss += loss.item()
    
    train_loss /= len(train_loader)
    
    <span class="comment"># Validation</span>
    model.eval()
    val_loss = 0
    <span class="keyword">with</span> torch.no_grad():
        <span class="keyword">for</span> X_batch, y_batch <span class="keyword">in</span> test_loader:
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            val_loss += loss.item()
    
    val_loss /= len(test_loader)
    scheduler.step(val_loss)
    
    <span class="keyword">if</span> (epoch + 1) % 10 == 0:
        <span class="function">print</span>(<span class="string">f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}'</span>)
    
    <span class="comment"># Save best model</span>
    <span class="keyword">if</span> val_loss < best_loss:
        best_loss = val_loss
        torch.save(model.state_dict(), <span class="string">'best_lstm_model.pth'</span>)

<span class="comment"># Load best model and evaluate</span>
model.load_state_dict(torch.load(<span class="string">'best_lstm_model.pth'</span>))
model.eval()

<span class="comment"># Make predictions</span>
<span class="keyword">with</span> torch.no_grad():
    predictions = model(X_test).cpu().numpy()
    actual = y_test.cpu().numpy()
    
    <span class="comment"># Inverse transform to original scale</span>
    predictions = scaler.inverse_transform(predictions)
    actual = scaler.inverse_transform(actual)
    
    <span class="comment"># Calculate metrics</span>
    mse = np.mean((predictions - actual) ** 2)
    mae = np.mean(np.abs(predictions - actual))
    <span class="function">print</span>(<span class="string">f'\nTest MSE: {mse:.4f}'</span>)
    <span class="function">print</span>(<span class="string">f'Test MAE: {mae:.4f}'</span>)</pre>
                </div>
                
                <div class="explanation">
                    <h3>ðŸ“– Explanation</h3>
                    <p><strong>LSTM (Long Short-Term Memory)</strong> networks are specialized RNNs designed for time series data, capable of learning long-term dependencies.</p>
                    <ul>
                        <li><strong>Sequence Creation:</strong> Creates sliding windows of historical data</li>
                        <li><strong>TimeSeriesSplit:</strong> Proper cross-validation that respects temporal order</li>
                        <li><strong>LSTM Architecture:</strong> Memory cells maintain information across time steps</li>
                        <li><strong>Gradient Clipping:</strong> Prevents exploding gradients in RNNs</li>
                        <li><strong>Multi-output Prediction:</strong> Predicts 3 values simultaneously</li>
                    </ul>
                    
                    <div class="models-box">
                        <h4>ðŸ”§ Time Series Models:</h4>
                        <ul>
                            <li><strong>LSTM:</strong> Handles long-term dependencies, best for complex patterns</li>
                            <li><strong>GRU (Gated Recurrent Unit):</strong> Simpler than LSTM, faster training</li>
                            <li><strong>Bi-directional LSTM:</strong> Processes sequences forward and backward</li>
                            <li><strong>Transformer (Temporal Fusion Transformer):</strong> State-of-the-art for time series</li>
                            <li><strong>TCN (Temporal Convolutional Network):</strong> CNN-based, parallelizable</li>
                            <li><strong>Prophet:</strong> Facebook's forecasting tool for business time series</li>
                            <li><strong>ARIMA/SARIMA:</strong> Classical statistical methods</li>
                        </ul>
                        
                        <h4>ðŸ’¡ Time Series Cross-Validation (Group CV):</h4>
                        <ul>
                            <li><strong>TimeSeriesSplit:</strong> Respects temporal order, no data leakage</li>
                            <li><strong>Rolling Window:</strong> Fixed-size training window moves forward</li>
                            <li><strong>Expanding Window:</strong> Training set grows, always uses all past data</li>
                            <li><strong>Blocked CV:</strong> Creates non-overlapping time blocks</li>
                        </ul>
                        
                        <h4>ðŸ” Important Considerations:</h4>
                        <ul>
                            <li>Never shuffle time series data during training</li>
                            <li>Handle seasonality and trends appropriately</li>
                            <li>Check for stationarity (constant mean/variance)</li>
                            <li>Be careful of data leakage in feature engineering</li>
                            <li>Use appropriate evaluation metrics (MAE, RMSE, MAPE)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Bonus: GRU Time Series Model -->
            <div class="section">
                <h2><span class="section-number">9</span>Bonus: GRU for Time Series</h2>
                <div class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn

<span class="comment"># Set device</span>
device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)

<span class="comment"># Define GRU model</span>
<span class="keyword">class</span> <span class="function">GRUModel</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.2):
        super(GRUModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.gru = nn.GRU(input_dim, hidden_dim, num_layers,
                         batch_first=True, dropout=dropout)
        
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, output_dim)
        )
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        out, _ = self.gru(x, h0)
        out = self.fc(out[:, -1, :])
        <span class="keyword">return</span> out

<span class="comment"># Initialize model (use same data preparation as LSTM example)</span>
model = GRUModel(input_dim=3, hidden_dim=128, num_layers=2, output_dim=3).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

<span class="function">print</span>(<span class="string">"GRU model initialized. Use same training loop as LSTM example."</span>)</pre>
                </div>
                
                <div class="explanation">
                    <h3>ðŸ“– Explanation</h3>
                    <p><strong>GRU (Gated Recurrent Unit)</strong> is a simpler alternative to LSTM with fewer parameters and faster training.</p>
                    <ul>
                        <li><strong>Simpler Architecture:</strong> Only 2 gates (reset and update) vs LSTM's 3</li>
                        <li><strong>Faster Training:</strong> Fewer parameters mean faster computation</li>
                        <li><strong>Similar Performance:</strong> Often performs comparably to LSTM</li>
                        <li><strong>Better for Shorter Sequences:</strong> LSTM better for very long sequences</li>
                    </ul>
                </div>
            </div>

            <!-- Bonus: Attention Mechanism -->
            <div class="section">
                <h2><span class="section-number">10</span>Bonus: LSTM with Attention</h2>
                <div class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F

<span class="comment"># Set device</span>
device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)

<span class="comment"># Attention mechanism</span>
<span class="keyword">class</span> <span class="function">Attention</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, hidden_dim):
        super(Attention, self).__init__()
        self.attention = nn.Linear(hidden_dim, 1)
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, lstm_output):
        <span class="comment"># lstm_output shape: (batch, seq_len, hidden_dim)</span>
        attention_weights = F.softmax(self.attention(lstm_output), dim=1)
        context_vector = torch.sum(attention_weights * lstm_output, dim=1)
        <span class="keyword">return</span> context_vector, attention_weights

<span class="comment"># LSTM with Attention</span>
<span class="keyword">class</span> <span class="function">LSTMAttention</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.2):
        super(LSTMAttention, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,
                           batch_first=True, dropout=dropout)
        self.attention = Attention(hidden_dim)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, output_dim)
        )
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        
        <span class="comment"># LSTM output for all time steps</span>
        lstm_out, _ = self.lstm(x, (h0, c0))
        
        <span class="comment"># Apply attention</span>
        context_vector, attention_weights = self.attention(lstm_out)
        
        <span class="comment"># Final prediction</span>
        out = self.fc(context_vector)
        <span class="keyword">return</span> out

<span class="comment"># Initialize model</span>
model = LSTMAttention(input_dim=3, hidden_dim=128, num_layers=2, output_dim=3).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

<span class="function">print</span>(<span class="string">"LSTM with Attention initialized. Use same training loop as previous examples."</span>)</pre>
                </div>
                
                <div class="explanation">
                    <h3>ðŸ“– Explanation</h3>
                    <p><strong>Attention Mechanism</strong> allows the model to focus on the most relevant time steps when making predictions.</p>
                    <ul>
                        <li><strong>Weighted Combination:</strong> Each time step gets an importance weight</li>
                        <li><strong>Better Long Sequences:</strong> Helps model focus on relevant information</li>
                        <li><strong>Interpretability:</strong> Attention weights show which time steps matter</li>
                        <li><strong>Improved Accuracy:</strong> Often outperforms vanilla LSTM</li>
                    </ul>
                    
                    <div class="models-box">
                        <h4>ðŸ”§ Advanced Time Series Techniques:</h4>
                        <ul>
                            <li><strong>Multi-head Attention:</strong> Multiple attention mechanisms in parallel</li>
                            <li><strong>Seq2Seq with Attention:</strong> For multi-step forecasting</li>
                            <li><strong>Autoencoder LSTM:</strong> Unsupervised feature learning</li>
                            <li><strong>Ensemble Methods:</strong> Combine multiple models</li>
                        </ul>
                    </div>
                </div>
            </div>

        </div>

        <footer>
            <p><strong>ðŸŽ¯ Summary:</strong> This guide covers 10 complete PyTorch implementations from basic regression to advanced time series forecasting.</p>
            <p><strong>ðŸ’» All code is production-ready and uses GPU when available.</strong></p>
            <p>Created with â¤ï¸ for Machine Learning Practitioners | Â© 2025</p>
        </footer>
    </div>
</body>
</html>